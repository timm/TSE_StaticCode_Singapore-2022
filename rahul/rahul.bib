@article{yang2021learning,
  title={Learning to recognize actionable static code warnings (is intrinsically easy)},
  author={Yang, Xueqi and Chen, Jianfeng and Yedida, Rahul and Yu, Zhe and Menzies, Tim},
  journal={Empirical Software Engineering},
  volume={26},
  number={3},
  pages={1--24},
  year={2021},
  publisher={Springer}
}

@article{frugal,
  author    = {H. Tu and
               T. Menzies},
  title     = {{FRUGAL:} Unlocking {SSL} for Software Analytics},
  journal   = {ASE},
  year      = {2021},
}

@article{debtfree,
  author    = {H. Tu and
               T. Menzies},
  title     = {{DebtFree:} minimizing labeling cost in self-admitted technical debt identification using semi-supervised learning
},
  journal   = {EMSE},
  year      = {2022},
}



@article{fairssl,
  author    = {J. Chakraborty and
               S. Majumder and
               H. Tu},
  title     = {Can We Achieve Fairness Using Semi-Supervised Learning?},
  journal   = {Fairware},
  year      = {2022},
}

@article{jitterbug,
  title={Identifying self-admitted technical debts with jitterbug: A two-step approach},
  author={Yu, Zhe and Fahid, Fahmid Morshed and Tu, Huy and Menzies, Tim},
  journal={IEEE Transactions on Software Engineering},
  year={2020},
  publisher={IEEE}
}

@misc{kumar20,
title="SVM RBF Kernel Parameters With Code Examples",
author="Ajitesh Kumar",
date="July 28, 2020",
note="Available on-line at \url{https://dzone.com/articles/using-jsonb-in-postgresql-how-to-effectively-store-1}"}

@article{kang2022detecting,
  title={Detecting False Alarms from Automatic Static Analysis Tools: How Far are We?},
  author={Kang, Hong Jin and Aw, Khai Loong and Lo, David},
  journal={arXiv preprint arXiv:2202.05982},
  year={2022}
}
@inproceedings{fu2017easy,
  title={Easy over hard: A case study on deep learning},
  author={Fu, Wei and Menzies, Tim},
  booktitle={Proceedings of the 2017 11th joint meeting on foundations of software engineering},
  pages={49--60},
  year={2017}
}

@article{Bergstra12,
author = {Bergstra, James and Bengio, Yoshua},
title = {Random Search for Hyper-Parameter Optimization},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
journal = {J. Mach. Learn. Res.},
month = {feb},
pages = {281–305},
numpages = {25},
keywords = {response surface modeling, neural networks, deep learning, model selection, global optimization}
}
@article{yedida2021value,
  title={On the value of oversampling for deep learning in software defect prediction},
  author={Yedida, Rahul and Menzies, Tim},
  journal={IEEE Transactions on Software Engineering},
  year={2021},
  publisher={IEEE}
}
@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}
@article{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{cordeiro2020survey,
  title={A Survey on Deep Learning with Noisy Labels: How to train your model when you cannot trust on the annotations?},
  author={Cordeiro, Filipe R and Carneiro, Gustavo},
  booktitle={2020 33rd SIBGRAPI conference on graphics, patterns and images (SIBGRAPI)},
  pages={9--16},
  year={2020},
  organization={IEEE}
}
@misc{barkan2021reduce,
  title={Reduce discrepancy of human annotators in medical imaging by automatic visual comparison to similar cases},
  author={Barkan, Ella and Hazan, Alon and Ratner, Vadim},
  year={2021},
  month=feb # "~9",
  publisher={Google Patents},
  note={US Patent 10,916,343}
}
@inproceedings{ma2019blind,
  title={Blind image quality assessment by learning from multiple annotators},
  author={Ma, Kede and Liu, Xuelin and Fang, Yuming and Simoncelli, Eero P},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)},
  pages={2344--2348},
  year={2019},
  organization={IEEE}
}
@book{mcnicol2005primer,
  title={A primer of signal detection theory},
  author={McNicol, Don},
  year={2005},
  publisher={Psychology Press}
}
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{agrawal2018better,
  title={Is" Better Data" Better Than" Better Data Miners"?},
  author={Agrawal, Amritanshu and Menzies, Tim},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
  pages={1050--1061},
  year={2018},
  organization={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{agrawal2021simpler,
  title={Simpler hyperparameter optimization for software analytics: why, how, when},
  author={Agrawal, Amritanshu and Yang, Xueqi and Agrawal, Rishabh and Yedida, Rahul and Shen, Xipeng and Menzies, Tim},
  journal={IEEE Transactions on Software Engineering},
  year={2021},
  publisher={IEEE}
}
@article{agrawal2019dodge,
  title={How to “dodge” complex software analytics},
  author={Agrawal, Amritanshu and Fu, Wei and Chen, Di and Shen, Xipeng and Menzies, Tim},
  journal={IEEE Transactions on Software Engineering},
  volume={47},
  number={10},
  pages={2182--2194},
  year={2019},
  publisher={IEEE}
}
@inproceedings{menzies2018500+,
  title={500+ times faster than deep learning:(a case study exploring faster methods for text mining stackoverflow)},
  author={Menzies, Tim and Majumder, Suvodeep and Balaji, Nikhila and Brey, Katie and Fu, Wei},
  booktitle={2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)},
  pages={554--563},
  year={2018},
  organization={IEEE}
}
@article{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization (2016)},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2017}
}
@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}
@article{santurkar2018does,
  title={How does batch normalization help optimization?},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{yedida2021lipschitzlr,
  title={Lipschitzlr: Using theoretically computed adaptive learning rates for fast convergence},
  author={Yedida, Rahul and Saha, Snehanshu and Prashanth, Tejas},
  journal={Applied Intelligence},
  volume={51},
  number={3},
  pages={1460--1478},
  year={2021},
  publisher={Springer}
}
@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@article{wang2018deep,
  title={Deep semantic feature learning for software defect prediction},
  author={Wang, Song and Liu, Taiyue and Nam, Jaechang and Tan, Lin},
  journal={IEEE Transactions on Software Engineering},
  volume={46},
  number={12},
  pages={1267--1293},
  year={2018},
  publisher={IEEE}
}
@inproceedings{li2017cclearner,
  title={Cclearner: A deep learning-based clone detection approach},
  author={Li, Liuqing and Feng, He and Zhuang, Wenjie and Meng, Na and Ryder, Barbara},
  booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={249--260},
  year={2017},
  organization={IEEE}
}
@inproceedings{white2015deep,
  title={Deep representations for software engineering},
  author={White, Martin},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  volume={2},
  pages={781--783},
  year={2015},
  organization={IEEE}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}
@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}
@article{leshno1993multilayer,
  title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  journal={Neural networks},
  volume={6},
  number={6},
  pages={861--867},
  year={1993},
  publisher={Elsevier}
}
@article{dasgupta1992power,
  title={The power of approximating: a comparison of activation functions},
  author={DasGupta, Bhaskar and Schnitger, Georg},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}
@article{zhu2005semi,
  title={Semi-supervised learning literature survey},
  author={Zhu, Xiaojin Jerry},
  year={2005},
  publisher={University of Wisconsin-Madison Department of Computer Sciences}
}
@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{zhai2019s4l,
  title={S4l: Self-supervised semi-supervised learning},
  author={Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1476--1485},
  year={2019}
}
@article{kingma2014semi,
  title={Semi-supervised learning with deep generative models},
  author={Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}
@inproceedings{kim2007warnings,
  title={Which warnings should I fix first?},
  author={Kim, Sunghun and Ernst, Michael D},
  booktitle={Proceedings of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
  pages={45--54},
  year={2007}
}